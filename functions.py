import streamlit as st
import os
import time
from pathlib import Path
import wave
import pyaudio
from pydub import AudioSegment
from audiorecorder import audiorecorder
import numpy as np
from scipy.io.wavfile import write
from langchain.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
)
from langchain.schema import SystemMessage
from langchain.memory import ConversationSummaryBufferMemory
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationChain
import constants as ct
import sounddevice as sd
import numpy as np
import wave

#def record_audio(audio_input_file_path):
#    """
#    éŸ³å£°å…¥åŠ›ã‚’å—ã‘å–ã£ã¦éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
#    """

#    audio = audiorecorder(
#        start_prompt="ç™ºè©±é–‹å§‹",
#        pause_prompt="ã‚„ã‚Šç›´ã™",
#        stop_prompt="ç™ºè©±çµ‚äº†",
#        start_style={"color":"white", "background-color":"black"},
#        pause_style={"color":"gray", "background-color":"white"},
#        stop_style={"color":"white", "background-color":"black"}
#    )

#    if len(audio) > 0:
#        audio.export(audio_input_file_path, format="wav")
#    else:
#        st.stop()


def record_audio(audio_input_file_path, samplerate=16000, channels=1, silence_threshold=0.02, silence_duration=5):
    """
    ç„¡éŸ³ãŒ silence_duration ç§’ç¶šã„ãŸã‚‰è‡ªå‹•åœæ­¢ã™ã‚‹éŒ²éŸ³é–¢æ•°ï¼ˆStreamlitå¯¾å¿œãƒ»å®Ÿæ™‚é–“ã‚«ã‚¦ãƒ³ãƒˆç‰ˆï¼‰
    - æœ€å¾Œã«éŸ³ãŒã‚ã£ãŸæ™‚åˆ»ã‚’åŸºæº–ã«ç„¡éŸ³æ™‚é–“ã‚’ç®—å‡ºã™ã‚‹ãŸã‚ã€ç§’æ•°ã‚«ã‚¦ãƒ³ãƒˆãŒæ­£ç¢ºã«ãªã‚Šã¾ã™ã€‚
    Args:
        audio_input_file_path: å‡ºåŠ›WAVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        samplerate: ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ
        channels: ãƒãƒ£ãƒ³ãƒãƒ«æ•°
        silence_threshold: ç„¡éŸ³åˆ¤å®šã®é–¾å€¤ï¼ˆfloat 0.0~1.0ï¼‰
        silence_duration: ç„¡éŸ³ãŒç¶šãç§’æ•°ã§éŒ²éŸ³åœæ­¢
    """

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs(os.path.dirname(audio_input_file_path), exist_ok=True)

    st.info("ğŸ¤ éŒ²éŸ³é–‹å§‹ã€‚ç™ºè©±å¾Œã€5ç§’é–“æ²ˆé»™ã™ã‚‹ã¨è‡ªå‹•ã§åœæ­¢ã—ã¾ã™ã€‚")

    recorded_data = []
    amplitude_list = []  # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯â†’ãƒ«ãƒ¼ãƒ—é–“ã§æœ€æ–°ã®æŒ¯å¹…ã‚’å…±æœ‰ã™ã‚‹ãŸã‚ã®ãƒªã‚¹ãƒˆ
    start_time = time.time()
    # æœ€å¾Œã«ã€ŒéŸ³ã‚ã‚Šã€ã¨åˆ¤å®šã—ãŸæ™‚åˆ»ï¼ˆåˆæœŸå€¤ã¯é–‹å§‹æ™‚åˆ»ï¼‰
    last_sound_time = [start_time]  # mutable container ã‚’ä½¿ã£ã¦ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‹ã‚‰æ›¸ãæ›ãˆå¯èƒ½ã«ã™ã‚‹

    # grace period: éŒ²éŸ³é–‹å§‹ç›´å¾Œã®å®‰å®šåŒ–æ™‚é–“ï¼ˆã“ã®é–“ã¯ç„¡éŸ³åˆ¤å®šã‚’è¡Œã‚ãªã„ï¼‰
    grace_period = 0.8  # 0.8ç§’ç¨‹åº¦ãŒå®‰å®šã—ã‚„ã™ã„ã§ã™ã€‚å¿…è¦ã«å¿œã˜ã¦èª¿æ•´ã—ã¦ãã ã•ã„ã€‚

    # UI è¦ç´ 
    progress_bar = st.progress(0)
    status_text = st.empty()

    chunk_duration = 0.1  # ãƒ«ãƒ¼ãƒ—ã®sleepé–“éš”ï¼ˆçŸ­ã‚ã«ã™ã‚‹ã¨UIãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒè‰¯ããªã‚‹ï¼‰
    # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆéŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®å—ã‘å–ã‚Šã¯ã“ã“ã§è¡Œã†ï¼‰
    def callback(indata, frames, time_info, status):
        # indata ã¯ float32 ã® -1.0 .. 1.0 ã‚¹ã‚±ãƒ¼ãƒ«ãŒæœŸå¾…ã•ã‚Œã‚‹
        recorded_data.append(indata.copy())
        amplitude = float(np.max(np.abs(indata)))
        amplitude_list.append(amplitude)
        # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å†…ã§ã¯æ™‚åˆ»ã ã‘æ›´æ–°ï¼ˆUIæ“ä½œã¯ã—ãªã„ï¼‰
        # å®Ÿéš›ã® last_sound_time æ›´æ–°ã¯ã€å¾Œæ®µã® if amplitude >= threshold ã§è¡Œã†ã‹ã€ã“ã“ã§ã‚‚è¡Œã£ã¦è‰¯ã„
        if amplitude >= silence_threshold:
            last_sound_time[0] = time.time()

    # éŒ²éŸ³ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’é–‹ã„ã¦ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—ã§ UI æ›´æ–°ã¨ç„¡éŸ³ç§’æ•°åˆ¤å®šã‚’è¡Œã†
    with sd.InputStream(samplerate=samplerate, channels=channels, callback=callback):
        while True:
            time.sleep(chunk_duration)  # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å‹•ã
            now = time.time()

            # ã¾ã  grace_period å†…ãªã‚‰ç„¡éŸ³æ™‚é–“ã¯ 0 ã«ã—ã¦çµŒéã‚’è¡¨ç¤ºã™ã‚‹
            if now - start_time < grace_period:
                silent_time = 0.0
            else:
                silent_time = now - last_sound_time[0]

            # UI ã‚’ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ¬ãƒƒãƒ‰å´ã§å®‰å…¨ã«æ›´æ–°
            last_amp = amplitude_list[-1] if amplitude_list else 0.0
            status_text.text(f"éŸ³é‡: {last_amp:.3f} / ç„¡éŸ³ {silent_time:.2f} ç§’")
            progress_bar.progress(min(int((silent_time / silence_duration) * 100), 100))

            # ç„¡éŸ³ãŒæŒ‡å®šç§’æ•°ç¶šã„ãŸã‚‰çµ‚äº†
            if silent_time >= silence_duration:
                break

    st.success(f"âœ… éŒ²éŸ³çµ‚äº†ï¼ˆ{silence_duration}ç§’é–“ç„¡éŸ³ã‚’æ¤œçŸ¥ï¼‰")

    # WAVä¿å­˜ï¼ˆfloat32 -> int16ï¼‰
    recorded_data_np = np.concatenate(recorded_data)
    # ã‚‚ã—ã‚¹ãƒ†ãƒ¬ã‚ªã§ shape (N,2) ã®å ´åˆã€wave.writeframes ã¯ interleaved int16 ã‚’æœŸå¾…ã™ã‚‹ã®ã§å¤‰æ›ã™ã‚‹
    # recorded_data_np ã¯ float32 ã®ç¯„å›² [-1,1]
    with wave.open(audio_input_file_path, 'w') as wf:
        wf.setnchannels(channels)
        wf.setsampwidth(2)  # 16-bit
        wf.setframerate(samplerate)
        wf.writeframes((recorded_data_np * 32767).astype(np.int16).tobytes())

    # UI ã®ç‰‡ä»˜ã‘
    progress_bar.empty()
    status_text.empty()

    return audio_input_file_path

def transcribe_audio(audio_input_file_path):
    """
    éŸ³å£°å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
    Args:
        audio_input_file_path: éŸ³å£°å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    """

    with open(audio_input_file_path, 'rb') as audio_input_file:
        transcript = st.session_state.openai_obj.audio.transcriptions.create(
            model="whisper-1",
            file=audio_input_file,
            language="en"
        )
    
    # éŸ³å£°å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    os.remove(audio_input_file_path)

    return transcript

def save_to_wav(llm_response_audio, audio_output_file_path):
    """
    ä¸€æ—¦mp3å½¢å¼ã§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå¾Œã€wavå½¢å¼ã«å¤‰æ›
    Args:
        llm_response_audio: LLMã‹ã‚‰ã®å›ç­”ã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿
        audio_output_file_path: å‡ºåŠ›å…ˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """

    temp_audio_output_filename = f"{ct.AUDIO_OUTPUT_DIR}/temp_audio_output_{int(time.time())}.mp3"
    with open(temp_audio_output_filename, "wb") as temp_audio_output_file:
        temp_audio_output_file.write(llm_response_audio)
    
    audio_mp3 = AudioSegment.from_file(temp_audio_output_filename, format="mp3")
    audio_mp3.export(audio_output_file_path, format="wav")

    # éŸ³å£°å‡ºåŠ›ç”¨ã«ä¸€æ™‚çš„ã«ä½œã£ãŸmp3ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    os.remove(temp_audio_output_filename)

def play_wav(audio_output_file_path, speed=1.0):
    """
    éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿ä¸Šã’
    Args:
        audio_output_file_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        speed: å†ç”Ÿé€Ÿåº¦ï¼ˆ1.0ãŒé€šå¸¸é€Ÿåº¦ã€0.5ã§åŠåˆ†ã®é€Ÿã•ã€2.0ã§å€é€Ÿãªã©ï¼‰
    """

    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    audio = AudioSegment.from_wav(audio_output_file_path)
    
    # é€Ÿåº¦ã‚’å¤‰æ›´
    if speed != 1.0:
        # frame_rateã‚’å¤‰æ›´ã™ã‚‹ã“ã¨ã§é€Ÿåº¦ã‚’èª¿æ•´
        modified_audio = audio._spawn(
            audio.raw_data, 
            overrides={"frame_rate": int(audio.frame_rate * speed)}
        )
        # å…ƒã®frame_rateã«æˆ»ã™ã“ã¨ã§æ­£å¸¸å†ç”Ÿã•ã›ã‚‹ï¼ˆãƒ”ãƒƒãƒã‚’ä¿æŒã—ãŸã¾ã¾é€Ÿåº¦ã ã‘å¤‰æ›´ï¼‰
        modified_audio = modified_audio.set_frame_rate(audio.frame_rate)

        modified_audio.export(audio_output_file_path, format="wav")

    # PyAudioã§å†ç”Ÿ
    with wave.open(audio_output_file_path, 'rb') as play_target_file:
        p = pyaudio.PyAudio()
        stream = p.open(
            format=p.get_format_from_width(play_target_file.getsampwidth()),
            channels=play_target_file.getnchannels(),
            rate=play_target_file.getframerate(),
            output=True
        )

        data = play_target_file.readframes(1024)
        while data:
            stream.write(data)
            data = play_target_file.readframes(1024)

        stream.stop_stream()
        stream.close()
        p.terminate()
    
    # LLMã‹ã‚‰ã®å›ç­”ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    os.remove(audio_output_file_path)

from langchain.chains import ConversationChain
from langchain.prompts import (
    ChatPromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
    SystemMessagePromptTemplate
)
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationSummaryBufferMemory

#ChatOpenAIã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å—ã‘å–ã‚‹ã‚ˆã†ã«ä¿®æ­£
def create_chain(template, llm=None):
    if llm is None:
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0.5)

    prompt = ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template(template),
        MessagesPlaceholder(variable_name="history"),
        HumanMessagePromptTemplate.from_template("{input}")
    ])

    memory = ConversationSummaryBufferMemory(llm=llm, return_messages=True)
    chain = ConversationChain(llm=llm, prompt=prompt, memory=memory)
    return chain

def create_problem_and_play_audio():
    """
    å•é¡Œç”Ÿæˆã¨éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®å†ç”Ÿ
    Args:
        chain: å•é¡Œæ–‡ç”Ÿæˆç”¨ã®Chain
        speed: å†ç”Ÿé€Ÿåº¦ï¼ˆ1.0ãŒé€šå¸¸é€Ÿåº¦ã€0.5ã§åŠåˆ†ã®é€Ÿã•ã€2.0ã§å€é€Ÿãªã©ï¼‰
        openai_obj: OpenAIã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    """

    # å•é¡Œæ–‡ã‚’ç”Ÿæˆã™ã‚‹Chainã‚’å®Ÿè¡Œã—ã€å•é¡Œæ–‡ã‚’å–å¾—
    problem = st.session_state.chain_create_problem.predict(input="")

    # LLMã‹ã‚‰ã®å›ç­”ã‚’éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
    llm_response_audio = st.session_state.openai_obj.audio.speech.create(
        model="tts-1",
        voice="alloy",
        input=problem
    )

    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
    audio_output_file_path = f"{ct.AUDIO_OUTPUT_DIR}/audio_output_{int(time.time())}.wav"
    save_to_wav(llm_response_audio.content, audio_output_file_path)

    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿ä¸Šã’
    play_wav(audio_output_file_path, st.session_state.speed)

    return problem, llm_response_audio

def create_evaluation():
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å€¤ã®è©•ä¾¡ç”Ÿæˆ
    """

    llm_response_evaluation = st.session_state.chain_evaluation.predict(input="")

    return llm_response_evaluation